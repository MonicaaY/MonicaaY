# -*- coding: utf-8 -*-
"""Depression_predictor( with ANN, Random_forest, KNN, Naive_bayes).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18rzqHoJJlTKlcaR0Xu6BbYbyga2JBYb6
"""

import tensorflow as tf
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import confusion_matrix

dp = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/b_depressed.csv")

dp.head(5)

dp.info()

dp["no_lasting_investmen"] = dp["no_lasting_investmen"].fillna(method='ffill')

dp.info()

dp = dp.drop(labels={'Survey_id','Ville_id'}, axis = 1)

sns.countplot(x = 'depressed', data = dp)

#sns.pairplot(dp, hue = 'depressed')

fig = plt.subplots(figsize = (20,20))
sns.heatmap(dp.corr(), annot = True)

selected_features = {'sex',	'Age',	'Married',	'Number_children',	'education_level', 'incoming_salary',	'incoming_own_farm',	'incoming_business',	'incoming_no_business', 'farm_expenses',	'labor_primary',	'lasting_investment',	'no_lasting_investmen' }
X = dp[selected_features]

dp.info()

X.info()

y = dp['depressed']

scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)
X_scaled

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size = 0.2)

X_train.shape

X_test.shape

y_train.shape

y_test.shape

classifier_model = tf.keras.models.Sequential()
classifier_model.add(tf.keras.layers.Dense(units = 400, activation = 'relu', input_shape = (13, )))
classifier_model.add(tf.keras.layers.Dropout(0.3))
classifier_model.add(tf.keras.layers.Dense(units = 400, activation = 'relu'))
classifier_model.add(tf.keras.layers.Dropout(0.3))
classifier_model.add(tf.keras.layers.Dense(units = 1, activation = 'sigmoid'))

classifier_model.compile(optimizer = 'Adam', loss = 'binary_crossentropy', metrics = 'accuracy')

epochs_hist = classifier_model.fit(X_train, y_train, epochs = 500, batch_size = 512)

epochs_hist.history.keys()

eh = epochs_hist.history['loss']
eh2 = epochs_hist.history['accuracy']
plt.plot(eh2)
plt.plot(eh)
plt.title('Training loss and accuracy graph')
plt.xlabel('epochs')
plt.ylabel('loss and accuracy')
plt.legend({'accuracy', 'loss'})

y_predict = classifier_model.predict(X_test)

y_predict

evaluation = classifier_model.evaluate(X_test, y_test)
print('test accuracy:{}'.format(evaluation[1]))

from sklearn.linear_model import LogisticRegression
model1 = LogisticRegression()

model1.fit(X_train,y_train)

model1.score(X_test,y_test)

from sklearn.neighbors import KNeighborsClassifier

model2 = KNeighborsClassifier()
model2.fit(X_train,y_train)

model2.score(X_test, y_test)

from sklearn.naive_bayes import GaussianNB
model3 = GaussianNB()

model3.fit(X_test, y_test)
model3.score(X_test, y_test)

from sklearn.ensemble import RandomForestClassifier
model4 = RandomForestClassifier()
model4.fit(X_train, y_train)

model4.score(X_test, y_test)
